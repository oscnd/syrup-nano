{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3975e842c652caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c0af47227c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"an\",\n",
    "    \"acro\",\n",
    "    \"act\",\n",
    "    \"aer/o\",\n",
    "    \"agr/i/o\",\n",
    "    \"alg/o\",\n",
    "    \"ambi, amphi\",\n",
    "    \"ambul\",\n",
    "    \"ami/o\",\n",
    "    \"ana\",\n",
    "    \"andr/o\",\n",
    "    \"anim\",\n",
    "    \"ann/enn\",\n",
    "    \"ante\",\n",
    "    \"anth/o\",\n",
    "    \"anthrop/o\",\n",
    "    \"anti\",\n",
    "    \"apo, apho\",\n",
    "    \"aqu/a\",\n",
    "    \"arbor\",\n",
    "    \"arch/i\",\n",
    "    \"arch/a/i\",\n",
    "    \"arthr/o\",\n",
    "    \"art\",\n",
    "    \"astro, aster\",\n",
    "    \"aud/i/io\",\n",
    "    \"auto\",\n",
    "    \"avi/a\",\n",
    "    \"bar/o\",\n",
    "    \"bell/i\",\n",
    "    \"bene\",\n",
    "    \"bi/n\",\n",
    "    \"bibli/o\",\n",
    "    \"bio\",\n",
    "    \"blast/o\",\n",
    "    \"burs\",\n",
    "    \"calc\",\n",
    "    \"cand\",\n",
    "    \"capt, cept, ceive\",\n",
    "    \"cardi/o\",\n",
    "    \"carn/i\",\n",
    "    \"cata\",\n",
    "    \"caust, caut\",\n",
    "    \"cede, ceed, cess\",\n",
    "    \"celer\",\n",
    "    \"cent/i\",\n",
    "    \"centr/o/i\",\n",
    "    \"cephal/o\",\n",
    "    \"cerebr/o\",\n",
    "    \"cert\",\n",
    "    \"chrom/o\",\n",
    "    \"chromat/o\",\n",
    "    \"chros\",\n",
    "    \"chron/o\",\n",
    "    \"chrys/o\",\n",
    "    \"cide, cise\",\n",
    "    \"circum, circle\",\n",
    "    \"claim, clam\",\n",
    "    \"clar\",\n",
    "    \"clud, clus\",\n",
    "    \"cline\",\n",
    "    \"co\",\n",
    "    \"col\",\n",
    "    \"com\",\n",
    "    \"cogn/i\",\n",
    "    \"con\",\n",
    "    \"contra/o\",\n",
    "    \"corp/o\",\n",
    "    \"cosm/o\",\n",
    "    \"counter\",\n",
    "    \"cranio\",\n",
    "    \"cred\",\n",
    "    \"cruc\",\n",
    "    \"crypto\",\n",
    "    \"cumul\",\n",
    "    \"curr, curs\",\n",
    "    \"cycl\",\n",
    "    \"de\",\n",
    "    \"dec/a, deka\",\n",
    "    \"deci\",\n",
    "    \"dem/o\",\n",
    "    \"demi\",\n",
    "    \"dendr/o/i\",\n",
    "    \"dent, dont\",\n",
    "    \"derm/a\",\n",
    "    \"di/plo\",\n",
    "    \"di/s\",\n",
    "    \"dia\",\n",
    "    \"dict\",\n",
    "    \"domin\",\n",
    "    \"don/at\",\n",
    "    \"duc/t\",\n",
    "    \"du/o\",\n",
    "    \"dur\",\n",
    "    \"dyn/a/am\",\n",
    "    \"dys\",\n",
    "    \"ego\",\n",
    "    \"em, en\",\n",
    "    \"endo\",\n",
    "    \"enn/i, anni\",\n",
    "    \"en, in\",\n",
    "    \"ep/i\",\n",
    "    \"equ/i\",\n",
    "    \"erg/o\",\n",
    "    \"esth/aesth\",\n",
    "    \"ethno\",\n",
    "    \"eu\",\n",
    "    \"ex\",\n",
    "    \"extra, extro\",\n",
    "    \"fac/t\",\n",
    "    \"fer\",\n",
    "    \"fid\",\n",
    "    \"flect\",\n",
    "    \"flor/a, fleur\",\n",
    "    \"for\",\n",
    "    \"fore\",\n",
    "    \"form\",\n",
    "    \"fract, frag\",\n",
    "    \"fug\",\n",
    "    \"funct\",\n",
    "    \"fus\",\n",
    "    \"gastr/o\",\n",
    "    \"gen/o\",\n",
    "    \"gene/sis\"\n",
    "    \"geo\",\n",
    "    \"ger\",\n",
    "    \"giga\",\n",
    "    \"gon\",\n",
    "    \"gram\",\n",
    "    \"gran\",\n",
    "    \"graph/y\",\n",
    "    \"grat\",\n",
    "    \"gyn/o/e\",\n",
    "    \"gress, grad/e/i\",\n",
    "    \"hect/o, hecat\",\n",
    "    \"helic/o\",\n",
    "    \"heli/o\",\n",
    "    \"hemi\",\n",
    "    \"hem/o/a\",\n",
    "    \"hepa\",\n",
    "    \"hept/a\",\n",
    "    \"herbi\",\n",
    "    \"hetero\",\n",
    "    \"hex/a\",\n",
    "    \"histo\",\n",
    "    \"homo, homeo\",\n",
    "    \"hydr/o\",\n",
    "    \"hygr/o\",\n",
    "    \"hyper\",\n",
    "    \"hyp/o\",\n",
    "    \"iatr/o\",\n",
    "    \"icon/o\",\n",
    "    \"idio\",\n",
    "    \"ig, il, im, in, ir\",\n",
    "    \"imag\",\n",
    "    \"infra\",\n",
    "    \"inter\",\n",
    "    \"intra, intro\",\n",
    "    \"ir\",\n",
    "    \"iso\",\n",
    "    \"ject\",\n",
    "    \"jud\",\n",
    "    \"junct\",\n",
    "    \"juven\",\n",
    "    \"kilo\",\n",
    "    \"kine/t\",\n",
    "    \"mat/o\",\n",
    "    \"lab\",\n",
    "    \"lact/o\",\n",
    "    \"later\",\n",
    "    \"leuk/o, leuc/o\",\n",
    "    \"lex\",\n",
    "    \"liber\",\n",
    "    \"lingu\",\n",
    "    \"lip/o\",\n",
    "    \"lite, ite, lith/o\",\n",
    "    \"loc\",\n",
    "    \"log/o\",\n",
    "    \"loqu, locu\",\n",
    "    \"luc\",\n",
    "    \"lud, lus\",\n",
    "    \"lumin\",\n",
    "    \"lun/a/i\",\n",
    "    \"macro\",\n",
    "    \"magn/a/i\",\n",
    "    \"mal/e\",\n",
    "    \"man/i/u\",\n",
    "    \"mand\",\n",
    "    \"mania\",\n",
    "    \"mar/i\",\n",
    "    \"mater, matr/i\",\n",
    "    \"max\",\n",
    "    \"medi\",\n",
    "    \"mega\",\n",
    "    \"melan/o\",\n",
    "    \"memor/i\",\n",
    "    \"merge, mers\",\n",
    "    \"meso\",\n",
    "    \"meta\",\n",
    "    \"meter, metr/y\",\n",
    "    \"micro\",\n",
    "    \"mid\",\n",
    "    \"migr\",\n",
    "    \"milli\",\n",
    "    \"min/i\",\n",
    "    \"mis/o\",\n",
    "    \"miss, mit\",\n",
    "    \"mob\",\n",
    "    \"mon/o\",\n",
    "    \"mot, mov\",\n",
    "    \"morph/o\",\n",
    "    \"mort\",\n",
    "    \"multi\",\n",
    "    \"mut\",\n",
    "    \"my/o\",\n",
    "    \"narr\",\n",
    "    \"nat\",\n",
    "    \"nav\",\n",
    "    \"necr/o\",\n",
    "    \"neg\",\n",
    "    \"neo\",\n",
    "    \"nephr/o\",\n",
    "    \"neur/o\",\n",
    "    \"nom/in\",\n",
    "    \"non\",\n",
    "    \"not\",\n",
    "    \"noun, nunc\",\n",
    "    \"nov\",\n",
    "    \"numer\",\n",
    "    \"ob, op\",\n",
    "    \"oct/a/o\",\n",
    "    \"ocu\",\n",
    "    \"od\",\n",
    "    \"odor \",\n",
    "    \"omni\",\n",
    "    \"op/t/s\",\n",
    "    \"opt\",\n",
    "    \"ortho\",\n",
    "    \"osteo\",\n",
    "    \"out\",\n",
    "    \"over\",\n",
    "    \"oxi/oxy\",\n",
    "    \"pale/o\",\n",
    "    \"pan\",\n",
    "    \"para\",\n",
    "    \"para\",\n",
    "    \"pater, patr/i\",\n",
    "    \"path\",\n",
    "    \"ped/i/e\",\n",
    "    \"pel\",\n",
    "    \"pent/a\",\n",
    "    \"pept, peps\",\n",
    "    \"per\",\n",
    "    \"peri\",\n",
    "    \"phag/e\",\n",
    "    \"phil/o\",\n",
    "    \"phon/o/e/y\",\n",
    "    \"phot/o\",\n",
    "    \"phyll/o\",\n",
    "    \"phys\",\n",
    "    \"phyt/o/e\",\n",
    "    \"plas/t/m\",\n",
    "    \"plaud, plod, plaus, plos\",\n",
    "    \"pneum/o\",\n",
    "    \"pod/e\",\n",
    "    \"poli\",\n",
    "    \"poly\",\n",
    "    \"pon\",\n",
    "    \"pop\",\n",
    "    \"port\",\n",
    "    \"pos\",\n",
    "    \"post\",\n",
    "    \"pre\",\n",
    "    \"pro\",\n",
    "    \"prot/o\",\n",
    "    \"pseud/o\",\n",
    "    \"psych/o\",\n",
    "    \"pugn/a, pung\",\n",
    "    \"pul\",\n",
    "    \"purg\",\n",
    "    \"put\",\n",
    "    \"pyr/o\",\n",
    "    \"quad/r/ri\",\n",
    "    \"quart\",\n",
    "    \"quin/t\",\n",
    "    \"radic, radix\",\n",
    "    \"radio\",\n",
    "    \"ram/i\",\n",
    "    \"re\",\n",
    "    \"reg\",\n",
    "    \"retro\",\n",
    "    \"rhin/o\",\n",
    "    \"rhod/o\",\n",
    "    \"rid\",\n",
    "    \"rrh/ea/oea/ag\",\n",
    "    \"rub\",\n",
    "    \"rupt\",\n",
    "    \"san\",\n",
    "    \"scend\",\n",
    "    \"sci\",\n",
    "    \"scler/o\",\n",
    "    \"scop/e/y\",\n",
    "    \"scrib, script\",\n",
    "    \"se\",\n",
    "    \"sect\",\n",
    "    \"sed, sid, sess\",\n",
    "    \"self\",\n",
    "    \"semi\",\n",
    "    \"sept/i\",\n",
    "    \"serv\",\n",
    "    \"sex\",\n",
    "    \"sol\",\n",
    "    \"sol\",\n",
    "    \"somn/i\",\n",
    "    \"son\",\n",
    "    \"soph\",\n",
    "    \"spec/t, spic\",\n",
    "    \"sphere\",\n",
    "    \"spir\",\n",
    "    \"sta\",\n",
    "    \"stell\",\n",
    "    \"struct\",\n",
    "    \"sub\",\n",
    "    \"sum\",\n",
    "    \"super\",\n",
    "    \"sy/m/n/l/s\",\n",
    "    \"tact, tang\",\n",
    "    \"tax/o\",\n",
    "    \"techno\",\n",
    "    \"tel/e/o\",\n",
    "    \"temp/or\",\n",
    "    \"ten, tin, tent\",\n",
    "    \"ter, trit\",\n",
    "    \"term/ina\",\n",
    "    \"terr/a/i\",\n",
    "    \"tetra\",\n",
    "    \"the\",\n",
    "    \"the/o\",\n",
    "    \"therm/o\",\n",
    "    \"tort\",\n",
    "    \"tox\",\n",
    "    \"tract\",\n",
    "    \"trans\",\n",
    "    \"tri\",\n",
    "    \"ultra\",\n",
    "    \"un\",\n",
    "    \"uni\",\n",
    "    \"urb\",\n",
    "    \"vac\",\n",
    "    \"ven/t\",\n",
    "    \"ver/i\",\n",
    "    \"verb\",\n",
    "    \"vers, vert\",\n",
    "    \"vice\",\n",
    "    \"vid\",\n",
    "    \"vince, vic\",\n",
    "    \"vis, vid\",\n",
    "    \"viv/i, vit\",\n",
    "    \"voc/i\",\n",
    "    \"vol/i/u\",\n",
    "    \"vor, vour\",\n",
    "    \"xanth\",\n",
    "    \"xen/o\",\n",
    "    \"xer/o/i\",\n",
    "    \"xyl\",\n",
    "    \"zo/o\",\n",
    "    \"zyg/o\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc7349bccc2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each item\n",
    "results = []\n",
    "\n",
    "for item in data:\n",
    "    # split by comma to handle multiple variants\n",
    "    variants = [v.strip() for v in item.split(',')]\n",
    "\n",
    "    for variant in variants:\n",
    "        # check if the variant contains slashes\n",
    "        if '/' in variant:\n",
    "            # split by slash and remove consecutive spaces\n",
    "            parts = [re.sub(r'\\s+', ' ', p.strip()) for p in variant.split('/') if p.strip()]\n",
    "\n",
    "            # generate all combinations starting from first part\n",
    "            base = parts[0]\n",
    "\n",
    "            if len(base) >= 2:\n",
    "                results.append({\n",
    "                    \"text\": base,\n",
    "                    \"words\": [base]\n",
    "                })\n",
    "\n",
    "            # output cumulative combinations\n",
    "            for i in range(1, len(parts)):\n",
    "                compound = base + parts[i]\n",
    "                if len(base) >= 2:\n",
    "                    results.append({\n",
    "                        \"text\": compound,\n",
    "                        \"words\": [base, parts[i]]\n",
    "                    })\n",
    "        else:\n",
    "            if len(variant) >= 2:\n",
    "                results.append({\n",
    "                    \"text\": variant,\n",
    "                    \"words\": [variant]\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133b590547a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('../dataset/tokenizer/root.jsonl', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885c742b81f45cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
